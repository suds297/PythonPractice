{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e62938f",
   "metadata": {},
   "source": [
    "#### Importing relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2514137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pynancial_steering import Redshift\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33beaba4",
   "metadata": {},
   "source": [
    "#### Redshift connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e05c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Redshift\n",
    "lh = Redshift()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c622a",
   "metadata": {},
   "source": [
    "####  Getting input dates for observation window\n",
    "####  Then we calculate the max window we need to have i.e. observation end date +7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d314a841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-02\n",
      "2022-01-17\n",
      "2022-01-02 00:00:00\n",
      "2022-01-17 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#format dd/mm/yy\n",
    "start_date = '2022-01-02'\n",
    "# print(start_date)\n",
    "end_date = '2022-01-09'\n",
    "# print(end_date)\n",
    "\n",
    "date1 = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "date2 = datetime.strptime(end_date,\"%Y-%m-%d\")\n",
    "date3 = date2 + timedelta(days=8)\n",
    "\n",
    "window_start_date = date1.strftime(\"%Y-%m-%d\")\n",
    "window_end_date = date3.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(window_start_date)\n",
    "print(window_end_date)\n",
    "print(date1)\n",
    "print(date3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3f23a",
   "metadata": {},
   "source": [
    "Creating a list of dates from the observation window \\\n",
    "We will use this list to iterate over the observation window data later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6587c64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2022, 1, 2, 0, 0),\n",
       " datetime.datetime(2022, 1, 3, 0, 0),\n",
       " datetime.datetime(2022, 1, 4, 0, 0),\n",
       " datetime.datetime(2022, 1, 5, 0, 0),\n",
       " datetime.datetime(2022, 1, 6, 0, 0),\n",
       " datetime.datetime(2022, 1, 7, 0, 0),\n",
       " datetime.datetime(2022, 1, 8, 0, 0),\n",
       " datetime.datetime(2022, 1, 9, 0, 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list = [date1 + timedelta(days=x) for x in range (((date2 + timedelta(days=1))-date1).days)]\n",
    "date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc21931",
   "metadata": {},
   "source": [
    "Pulling data from lakehouse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lh=lh.run_query(\n",
    "    f\"COMMIT;\"\n",
    "    \n",
    "    f\"select occurred_at, user_attributes_market, customer_id, event_name from \"\n",
    "    f\"xx_yy.zz_kk \"\n",
    "    f\"where user_attributes_market='US' and event_name in \"\n",
    "    f\"('Help me', 'Entertain you')\"\n",
    "    f\" and occurred_at between '{date1}' and '{date3}' \"\n",
    "    f\"limit 100000\" \n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115fed8e",
   "metadata": {},
   "source": [
    "### Filtering and creating 2 separate data frames for each event and substring to create date from timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd899f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter first\n",
    "df_vcn = df_lh.loc[df_lh['event_name'] == 'Help me']\n",
    "df_vcn['amp_date'] = df_vcn['occurred_at'].astype(str).str.slice(0,10)\n",
    "\n",
    "\n",
    "df_tdp = df_lh.loc[df_lh['event_name'] == 'Entertain you']\n",
    "df_tdp['amp_date'] = df_tdp['occurred_at'].astype(str).str.slice(0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7437a",
   "metadata": {},
   "source": [
    "1. Now we iterate over the list of observation window dates one at a time  \n",
    "2. We first pass this date to our VCN dataframe and pull out all records for one date at a time \n",
    "3. Then we use this window date +7 to full all relevant records from the tDP dataframe  \n",
    "4. Nxt we sum up the number of times the Open TDP event happened at a user level  \n",
    "5. Left merge/join to the dataframe in step 2 \n",
    "6. Append/concatenate to empty (first iteration)/union file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fff16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(date_list)):\n",
    "    df_vcn_1 = df_vcn[df_vcn['amp_date'] == date_list[i].strftime(\"%Y-%m-%d\").drop(\"occurred_at\",1)\n",
    "                    \n",
    "    df_vcn_2 = df_vcn_1.drop_duplicates([\"customer_id\",\"event_name\",\"amp_date\"],keep=\"first\")\n",
    "                      \n",
    "    \n",
    "                      \n",
    "    df_tdp_1 = df_tdp[df_tdp['amp_date'] >= date_list[i].strftime(\"%Y-%m-%d\")\n",
    "                     & (df_tdp['amp_date'] <= (date_list[i] + timedelta(days=7)).strftime(\"%Y-%m-%d\"))]\n",
    "                      \n",
    "    df_tdp_2 = df_tdp_1.groupby([\"customer_id\"],sort=True)['event_name'].count().reset_index(name=\"count_tdp_view\")\n",
    "    \n",
    "    merge_1 = pd.merge(df_vcn_2.astype(str),df_tdp_2.astype(str), how = 'left', on = 'customer_id')\n",
    "                      \n",
    "    if i == 0:\n",
    "            df_union = merge_1\n",
    "    \n",
    "    else:\n",
    "        df_union = df_union.append(merge_1, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4fc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dace64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union['count_tdp_view'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d14d40",
   "metadata": {},
   "source": [
    "#### 1. Fillna to substitute Nan values with zeroes\n",
    "#### 2. Groupby to count number of users by number of times that the TDP event happened in the nxt 7 days\n",
    "#### 3. Correcting messed up formats\n",
    "#### 4. Sorting in ascending order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30599293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final table\n",
    "\n",
    "df_union['count_tdp_view'] = df_union['count_tdp_view'].fillna()\n",
    "df_final_1 = df_union.groupby(\"count_tdp_view\", sort=True)['customer_id'].count().reset_index(name=\"count_users\")\n",
    "df_final_1['count_tdp_view'] = df_final_1['count_tdp_view'].astype('int')\n",
    "df_final_1.sort_values(by=['count_tdp_view'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
